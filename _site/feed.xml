<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>YXZF&#39;s  Blog</title>
    <description>无风居士的博客</description>
    <link>http://leijun00.github.io/</link>
    <atom:link href="/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sun, 16 Apr 2017 20:08:10 +0800</pubDate>
    <lastBuildDate>Sun, 16 Apr 2017 20:08:10 +0800</lastBuildDate>
    <generator>Jekyll v2.4.0</generator>
    
      <item>
        <title>XGBoost解读(2)--近似分割算法</title>
        <description>&lt;h1 id=&quot;section&quot;&gt;近似分割算法&lt;/h1&gt;
&lt;p&gt;&lt;a href=&quot;https://yxzf.github.io/2017/03/xgboost-v1/&quot;&gt;XGBoost解读(1)–原理&lt;/a&gt;中介绍了XGBoost使用exact greedy算法来寻找分割点建树，但是当数据量非常大难以被全部加载进内存时或者分布式环境下时，exact greedy算法将不再合适。因此作者提出近似算法来寻找分割点。近似算法的大致流程见下面的算法。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/datamining/xgboost_fig/tree_split1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;对于某个特征$k$，算法首先根据特征分布的分位数找到切割点的候选集合&lt;script type=&quot;math/tex&quot;&gt;S_k = \{s_{k1}, s_{k2}, ... ,s_{kl} \}&lt;/script&gt;；然后将特征$k$的值根据集合$S_k$划分到桶(bucket)中，接着对每个桶内的样本统计值$G$、$H$进行累加统计，最后在这些累计的统计量上寻找最佳分裂点。从算法伪代码可以看出近似算法的核心是如何&lt;strong&gt;根据分位数采样得到分割点的候选集合$S$&lt;/strong&gt;.  本文接下来的内容也是围绕这个进行阐述。 &lt;/p&gt;

&lt;h1 id=&quot;quantile&quot;&gt;Quantile&lt;/h1&gt;
&lt;p&gt;## $\phi$-quantile&lt;br /&gt;
&lt;a href=&quot;http://www.mathcs.emory.edu/~cheung/Courses/584-StreamDB/Syllabus/08-Quantile/Greenwald.html&quot;&gt;Quantile&lt;/a&gt;就是ranking。如果有N个元素，那么$\phi$-quantile就是指rank在$\lfloor \phi \times N \rfloor$的元素。例如$S=[11, 21, 24, 61, 81, 39, 89, 56, 12, 51]$，首先排序为$[11, 12,  21,  24,  39,  51,  56,  61,  81,  89]$，则0.1-quantile=11, 0.5-quantile=39. 上面的是exact quantile寻找方法，如果数据集非常大，难以排序，则需要引入$\epsilon$-approximate $\phi$-quantiles&lt;/p&gt;

&lt;h2 id=&quot;epsilon-approximate-phi-quantiles&quot;&gt;$\epsilon$-approximate $\phi$-quantiles&lt;/h2&gt;
&lt;p&gt;也就是$\phi$-quantile是在区间$[ \lfloor (\phi - \epsilon) \times N \rfloor, \lfloor (\phi + \epsilon) \times N \rfloor]$。还是上面的例子，领$\epsilon=0.1$，则有0.2-quantile={11, 12, 21}&lt;/p&gt;

&lt;h1 id=&quot;weighted--datasets&quot;&gt;Weighted  Datasets&lt;/h1&gt;
&lt;p&gt;回到XGBoost的建树过程，在建立第$i$棵树的时候已经知道数据集在前面$i-1$棵树的误差，因此采样的时候是需要考虑误差，对于误差大的特征值采样粒度要加大，误差小的特征值采样粒度可以减小，也就是说采样的样本是需要权重的。&lt;/p&gt;

&lt;p&gt;重新审视目标函数&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\begin{equation}
\sum_{i=1}^n [g_i f_t(x_i) + \frac{1}{2} h_i f_t^2(x_i)] + \Omega(f_t)
\end{equation}
&lt;/script&gt;

&lt;p&gt;通过配方可以得到&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\begin{equation}
\sum_{1}^n \left[ \frac {1}{2}  h_i \left( f_t(x_i) - (-g_i/h_i)\right)^2 \right] + \Omega (f_t) + constant
\end{equation}
&lt;/script&gt;

&lt;p&gt;因此可以将该目标还是看作是关于标签为${-{g_i}/{h_i}}$和权重为$h_i$的平方误差形式。&lt;br /&gt;
&amp;gt; 论文中$\frac{g_i}{h_i}$前面没有负号，可是通过推导我认为这种形式才是对的。当然这边的符号不影响论文中其他表达的正确性&lt;/p&gt;

&lt;h2 id=&quot;h&quot;&gt;二阶导数h为权重的解释&lt;/h2&gt;
&lt;p&gt;如果损失函数是Square loss，即$Loss(y, \widehat y) = (y - \widehat y)^2$，则$h=2$，那么实际上是不带权。&lt;br /&gt;
如果损失函数是Log loss，则$h=pred* (1 - pred)$. 这是个开口朝下的一元二次函数，所以最大值在0.5。当pred在0.5附近，这个值是非常不稳定的，很容易误判，h作为权重则因此变大，那么直方图划分，这部分就会被切分的更细&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/datamining/xgboost_fig/weighted.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;问题转换&lt;/h2&gt;
&lt;p&gt;记&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\begin{equation}
D_k = \{(x_{1k}, h_1), (x_{2k}, h_2), \cdots (x_{nk}, h_n)\}
\end{equation}
&lt;/script&gt;

&lt;p&gt;表示 每个训练样本的第$k$维特征值和对应二阶导数。接下来定义排序函数为$r_k(\cdot):R \rightarrow[0, +\infty)$&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[

\begin{equation}
r_k (z) = \frac {1} {\sum\limits_{\left( {x,h} \right) \in {D_k}} h } \sum\limits_{\left( {x,h} \right) \in {D_k},x &lt; z} h 
\end{equation}
 %]]&gt;&lt;/script&gt;

&lt;p&gt;函数表示特征的值小于$z$的样本分布占比，其中二阶导数$h$可以视为权重，后面论述。在这个排序函数下，我们找到一组点&lt;script type=&quot;math/tex&quot;&gt; \{ s_{k1}, s_{k2}, ... ,s_{kl} \} &lt;/script&gt; ,满足：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\begin{equation}
s_{k,j}
\end{equation}
&lt;/script&gt;

&lt;p&gt;其中，${s&lt;em&gt;{k1}} = \mathop {\min }\limits_i {x&lt;/em&gt;{ik}},{s&lt;em&gt;{kl}} = \mathop {\max }\limits_i {x&lt;/em&gt;{ik}}$。$\varepsilon$为采样率，直观上理解，我们最后会得到$1/\varepsilon$个分界点。&lt;/p&gt;

&lt;p&gt;对于每个样本都有相同权重的问题，有quantile sketch算法解决该问题，作者提出Weighted Quantile Sketch算法解决这种weighted datasets的情况。具体算法描述和推理在论文的&lt;a href=&quot;http://homes.cs.washington.edu/~tqchen/pdf/xgboost-supp.pdf&quot;&gt;补充材料&lt;/a&gt;。&lt;/p&gt;

&lt;h1 id=&quot;weighted-quantile-sketch&quot;&gt;Weighted Quantile Sketch&lt;/h1&gt;
&lt;p&gt;## Formalization&lt;br /&gt;
给定一个multi-set &lt;script type=&quot;math/tex&quot;&gt;D = \left\{ {\left( ,{w_1}} \right),\left( ,{w_2}} \right) \cdots \left( ,{w_n}} \right)} \right\}$, $w_i\in [0, +\infty], x_i \in \mathcal{X}&lt;/script&gt;. 如果数据集D是根据$\mathcal{X}$上的升序进行排列，作者定义了两个rank function &lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
r_{D}^{-} = \sum_{(x, w)\in D, x&lt;y} w %]]&gt;&lt;/script&gt;, &lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;r_{D}^{+} = \sum_{(x, w)\in D, x\leq y} w&lt;/script&gt;&lt;br /&gt;
注意到$D$是个multi-set，因此会有一些数据具有相同的$x$和$w$，作者还定义了weight function &lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;w_{D}(y) = r_{D}^{+}(y) - r_{D}^{-}(y) = \sum_{(x,w)\in D, x=y}w&lt;/script&gt;. 样本集合D上的全部权重定义为&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;w(D) = \sum_{(x, w)\in D} w&lt;/script&gt;&lt;/p&gt;

&lt;h2 id=&quot;quantile-summary-of-weighted-data&quot;&gt;Quantile Summary of Weighted Data&lt;/h2&gt;
&lt;p&gt;根据上面定义的概念和符号引出Quantile Summary of Weighted Data的定义&lt;br /&gt;
数据集$D$上的quantile summary被定义为$Q(D) = (S, \tilde{r}&lt;em&gt;{D}^{+}, \tilde{r}&lt;/em&gt;{D}^{-}, \tilde{w}&lt;em&gt;{D})$, 其中集合$S={x_1,x_2,…, x_k}$从D中选出($x_i\in {x|(x_w)\in D}$). S中的元素需要满足下面的性质:&lt;br /&gt;
    1 $x_i &amp;lt; x&lt;/em&gt;{i+1}$ 对所有的$i$成立。并且$x_1$和$x_k$分别是$D$中的最小和最大点:&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;x_1=min_{(x,w)\in D} x \quad\quad x_k=max_{(x,w)\in D} x&lt;/script&gt;&lt;br /&gt;
    2 函数$\tilde{r}&lt;em&gt;{D}^{+}, \tilde{r}&lt;/em&gt;{D}^{-}, \tilde{w}&lt;em&gt;{D}$是定义在集合$S$上的函数，并且满足&lt;br /&gt;
$\tilde{r}&lt;/em&gt;{D}^{-}(x_i) \leq r&lt;em&gt;{D}^{-}(x_i)$， $\tilde{r}&lt;/em&gt;{D}^{+}(x_i) \leq r&lt;em&gt;{D}^{+}(x_i)$， $\tilde{w}&lt;/em&gt;{D}(x_i) \leq w_{D}(x_i)$&lt;/p&gt;

&lt;p&gt;$\tilde{r}&lt;em&gt;{D}^{-}(x_i) + \tilde{w}&lt;/em&gt;{S}(x_i) \leq \tilde{r}&lt;em&gt;{D}^{-} (x&lt;/em&gt;{i+1})$,  $\tilde{r}&lt;em&gt;{D}^{+}(x_i) \leq \tilde{r}&lt;/em&gt;{D}^{+}(x&lt;em&gt;{i+1}) - \tilde{w}&lt;/em&gt;{D}(x_{i+1})$&lt;/p&gt;

&lt;h2 id=&quot;varepsilon-approximate-qunatile-summary&quot;&gt;$\varepsilon$-Approximate Qunatile Summary&lt;/h2&gt;
&lt;p&gt;给定的quantile summary $Q(D)=(S, \tilde{r}&lt;em&gt;{D}^{+}, \tilde{r}&lt;/em&gt;{D}^{-}, \tilde{w}&lt;em&gt;{D})$, $Q(D)$被称为$\varepsilon$-Approximate Qunatile summay，当且仅当 $\tilde{r}&lt;/em&gt;{D}^{+}(y)-\tilde{r}&lt;em&gt;{D}^{-}(y)-\tilde{w}&lt;/em&gt;{D}(y) \leq \varepsilon w(S)$ 对于任意一个$y\in X$成立.&lt;br /&gt;
意思也就是说我们对$r&lt;em&gt;{y}^{+}$、$r&lt;/em&gt;{y}^{-}$的估计的最大error至多为$\varepsilon w(D)$&lt;/p&gt;

&lt;h2 id=&quot;varepsilon-approximate-qunatile-summary-1&quot;&gt;构建$\varepsilon$-Approximate Qunatile Summary&lt;/h2&gt;
&lt;p&gt;### 初始化&lt;br /&gt;
在小规模数据集$D = \left{ {\left( ,{w_1}} \right),\left( ,{w_2}} \right) \cdots \left( ,{w_n}} \right)} \right}$上构建初始的quantile summary $Q(D) = (S, \tilde{r}&lt;em&gt;{D}^{+}, \tilde{r}&lt;/em&gt;{D}^{-}, \tilde{w}&lt;em&gt;{D})$，集合$S$满足：$S={x|(x,w)\in D}$. $\tilde{r}&lt;/em&gt;{D}^{+}, \tilde{r}&lt;em&gt;{D}^{-}, \tilde{w}&lt;/em&gt;{D}$被定义为&lt;br /&gt;
$\tilde{r}&lt;em&gt;{D}^{-}(x) = r&lt;/em&gt;{D}^{-}(x)$， $\tilde{r}&lt;em&gt;{D}^{+}(x) = r&lt;/em&gt;{D}^{+}(x)$， $\tilde{w}&lt;em&gt;{D}(x) = w&lt;/em&gt;{D}(x)  \quad for \quad  x\in S$&lt;br /&gt;
可以看出，初始的$Q(D)$是0-approximate summary.&lt;/p&gt;

&lt;h3 id=&quot;merge-operation&quot;&gt;Merge Operation&lt;/h3&gt;
&lt;p&gt;$Q(D_1)=(S_1, \tilde{r}&lt;em&gt;{D_1}^{+}, \tilde{r}&lt;/em&gt;{D_1}^{-}, \tilde{w}&lt;em&gt;{D&lt;/em&gt;{1}})$和$Q(D_2)=(S_1, \tilde{r}&lt;em&gt;{D_2}^{+}, \tilde{r}&lt;/em&gt;{D_2}^{-}, \tilde{w}&lt;em&gt;{D&lt;/em&gt;{2}})$分别定义在数据集$D1$和$D2$上，令$D=D_1\cup D_2$，那么merged summary $Q(D)=(S, \tilde{r}&lt;em&gt;{D}^{+}, \tilde{r}&lt;/em&gt;{D}^{-}, \tilde{w}&lt;em&gt;D)$被定义为：&lt;br /&gt;
* $S={(x_1, x_2, …, x_k)}, \quad x_i\in S_1 \quad or \quad s_i \in S_2$&lt;br /&gt;
* $\tilde{r}&lt;/em&gt;{D}^{-}(x_i) = \tilde{r}&lt;em&gt;{D_1}^{-}(x_i) + \tilde{r}&lt;/em&gt;{D_2}^{-}(x_i) $&lt;br /&gt;
* $\tilde{r}&lt;em&gt;{D}^{+}(x_i) = \tilde{r}&lt;/em&gt;{D_1}^{+}(x_i) + \tilde{r}&lt;em&gt;{D_2}^{+}(x_i) $&lt;br /&gt;
* $\tilde{w}&lt;/em&gt;{D}(x_i) = \tilde{w}&lt;em&gt;{D_1}(x_i) + \tilde{w}&lt;/em&gt;{D_2}(x_i) $&lt;/p&gt;

&lt;h3 id=&quot;prune-operation&quot;&gt;Prune Operation&lt;/h3&gt;
&lt;p&gt;给定$Q(D)=(S, \tilde{r}&lt;em&gt;{D}^{+}, \tilde{r}&lt;/em&gt;{D}^{-}, \tilde{w}&lt;em&gt;{D})$(其中$S={x_1, x_2, …, x_k}$)和memory budget $b$，prune operation构建一个新的summary, $\acute{Q}(D)=(\acute{S}, \tilde{r}&lt;/em&gt;{D}^{+}, \tilde{r}&lt;em&gt;{D}^{-}, \tilde{w}&lt;/em&gt;{D})$. $\acute{D}$中的$\tilde{r}&lt;em&gt;{D}^{+}, \tilde{r}&lt;/em&gt;{D}^{-}, \tilde{w}&lt;em&gt;{D}$定义与原先的summary $Q$一致，只是定义域从$S$变为$\acute{S}$, $\acute{S}={\acute{x_1}, \acute{x_2}, …, \acute{x&lt;/em&gt;{b+1}}}$,  $\acute{x_i}$的选择按照下面的操作获取：&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\acute{x_i}=g(Q, \frac{i-1}{b} w(D))&lt;/script&gt;&lt;br /&gt;
$g(Q, d)$是query function，对于给定的quantile summary $Q$和rank $d$, $g(Q, d)$返回一个元素$x$，$x$的rank最接近$d$，具体定义为&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/datamining/xgboost_fig/query.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;section-2&quot;&gt;参考资料&lt;/h1&gt;
&lt;p&gt;http://datascience.stackexchange.com/questions/10997/need-help-understanding-xgboosts-approximate-split-points-proposal&lt;/p&gt;

</description>
        <pubDate>Sun, 16 Apr 2017 00:00:00 +0800</pubDate>
	<link>http://leijun00.github.io/2017/04/xgboost-v2/</link>
	<guid isPermaLink="true">http://leijun00.github.io/2017/04/xgboost-v2/</guid>
        
        <category>GBDT</category>
        
        <category>XGBoost</category>
        
        
        <category>datamining</category>
        
      </item>
    
      <item>
        <title>XGBoost解读(1)--原理</title>
        <description>&lt;h4 id=&quot;section&quot;&gt;1 集成学习&lt;/h4&gt;
&lt;p&gt;科学研究中，有种方法叫做组合，甚是强大，小硕们毕业基本靠它了。将别人的方法一起组合起来然后搞成一个集成的算法，集百家之长，效果一般不会差。其实也不能怪小硕们，大牛们也有这么做的，只是大牛们做的比较漂亮。&lt;/p&gt;

&lt;p&gt;在PAC学习框架下（Probably Approximately Correct）, Kearns和Valiant指出，若存在一个多项式级的学习算法来识别一组概念，并且识别正确率很高，那么这组概念是强可学习的；而如果学习算法识别一组概念的正确率仅比随机猜测略好，那么这组概念是弱可学习的。Schapire证明了弱学习算法与强学习算法的等价性问题，这样在学习概念时，只要找到一个比随机猜测略好的弱学习算法，就可以将其提升为强学习算法，而不必直接去找通常情况下很难获得的强学习算法。这为集成学习提供了理论支持。&lt;/p&gt;

&lt;p&gt;在众多单模型中（与集成模型相对应），决策树这种算法有着很多良好的特性，比如说训练时间复杂度较低，预测的过程比较快速，模型可解释强等。但是同时，单决策树又有一些不好的地方，比如说容易over-fitting，虽然有一些方法，如剪枝可以减少这种情况，但是还是不够的。&lt;/p&gt;

&lt;p&gt;集成学习还是有很多方法的，在这里只介绍两种普遍做法：bagging集成和boosting集成。bagging集成，例如随机森林，对样本随机抽样建立很多树，每棵树之间没有关联，这些树组成森林，构成随机森林模型。boosting集成后一个模型是对前一个模型产生误差的矫正。gradient boost更具体，是指每个新模型的引入是为了减少上个模型的残差(residual)，而为了消除残差，我们可以在残差减少的梯度(Gradient)方向上建立一个新的模型。如果基础模型是决策树，那么这样的模型就被称为Gradient Boost Decision Tree（GBDT）&lt;/p&gt;

&lt;h4 id=&quot;xgboost&quot;&gt;2 XGBoost&lt;/h4&gt;
&lt;p&gt;GBDT可以看做是个框架，最早的一种实现方法由Friedman 在论文GREEDY FUNCTION APPROXIMATION: A GRADIENT BOOSTING MACHINE 。xgboost（eXtreme Gradient Boosting）是最近提出的一个新的GBDT实现，由陈天奇提出，在Kaggle、KDD Cup等数据挖掘比赛中大方异彩，成为冠军队伍的标配，另外很多大公司，如腾讯、阿里、美团已在公司里面部署。XGBoost有如下优点:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;显示的把树模型复杂度作为正则项加到优化目标中。&lt;/li&gt;
  &lt;li&gt;公式推导中用到了二阶导数，用了二阶泰勒展开。&lt;/li&gt;
  &lt;li&gt;实现了分裂点寻找近似算法。&lt;/li&gt;
  &lt;li&gt;利用了特征的稀疏性。&lt;/li&gt;
  &lt;li&gt;数据事先排序并且以block形式存储，有利于并行计算。&lt;/li&gt;
  &lt;li&gt;基于分布式通信框架rabit，可以运行在MPI和yarn上。&lt;/li&gt;
  &lt;li&gt;实现做了面向体系结构的优化，针对cache和内存做了性能优化。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;section-1&quot;&gt;3 监督学习要素&lt;/h4&gt;
&lt;p&gt;XGBoost应用于监督学习问题，使用训练数据(有很多特征)$x_i$来预测目标$y_i$。 监督学习的三要素：模型、参数和目标函数&lt;/p&gt;

&lt;h5 id=&quot;section-2&quot;&gt;3.1 模型&lt;/h5&gt;
&lt;p&gt;模型指给定输入$x_i$如何去预测输出$y_i$。我们比较常见的模型如线性模型（包括线性回归和logistic regression）采用了线性叠加的方式进行预测$\hat{y}_i=\sum_j w_j x_{ij}$. 其实这里的预测$y$可以有不同的解释，比如我们可以用它来作为回归目标的输出，或者进行sigmoid 变换得到概率，或者作为排序的指标等。而一个线性模型根据$y$的解释不同（以及设计对应的目标函数）用到回归，分类或排序等场景。&lt;/p&gt;

&lt;h5 id=&quot;section-3&quot;&gt;3.2 参数&lt;/h5&gt;
&lt;p&gt;参数指我们需要学习的东西，在线性模型中，参数指我们的线性系数$w$。&lt;/p&gt;

&lt;h5 id=&quot;section-4&quot;&gt;3.3 目标函数&lt;/h5&gt;
&lt;p&gt;模型和参数本身指定了给定输入我们如何做预测，但是没有告诉我们如何去寻找一个比较好的参数，这个时候就需要目标函数登场了。一般的目标函数包含下面两项：&lt;br /&gt;
&lt;img src=&quot;/images/datamining/xgboost_fig/reg_1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;其中$L$为训练损失函数，$\Sigma$是正则项，惩罚模型复杂度。&lt;br /&gt;
常见的损失函数，对于回归问题，有损失函数L为最小平方误差: $L(y, \hat{y}) = (y - \hat{y})^2$，对于二分类，损失函数L为logit loss $L(y, \hat{y})=-log\,\sigma(y, \hat{y})$&lt;br /&gt;
为什么加入正则项？因为模型是在训练集上训练，但是实际应用时时另一份数据集，一般为测试集。那么模型在训练集上表现优异，不代表在测试集上会好。越是在训练集上模型越是复杂，则模型过拟合会比较严重。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/datamining/xgboost_fig/reg_3.png&quot; alt=&quot;&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/images/datamining/xgboost_fig/reg_2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;奥卡姆剃刀原则：在所有可能选择的模型中，能够很好解释已知数据，并且十分简单的模型才是最好的模型。总而言之，加入正则项是为了提高模型的泛化能力，既在未知数据集上同样表现良好。&lt;br /&gt;
常见的正则化项有L1正则和L2正则。L1正则项是参数向量的L1范数，L2正则项是参数向量的L2范数&lt;/p&gt;

&lt;h4 id=&quot;boosted-tree&quot;&gt;4  Boosted Tree&lt;/h4&gt;
&lt;p&gt;##### 4.1 基学习器CART&lt;br /&gt;
Boosted tree 最基本的组成部分叫做回归树(regression tree)，也叫做CART。&lt;br /&gt;
&lt;img src=&quot;/images/datamining/xgboost_fig/cart.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;上面就是一个CART的例子。CART会把输入根据输入的属性分配到各个叶子节点，而每个叶子节点上面都会对应一个实数分数，你可以把叶子的分数理解为有多可能这个人喜欢电脑游戏。&lt;br /&gt;
&amp;gt; CART树模型的参数是什么？树的参数一般为树的结构、树的叶子节点、叶子节点的值等&lt;/p&gt;

&lt;h5 id=&quot;tree-ensemble&quot;&gt;4.2 Tree Ensemble&lt;/h5&gt;
&lt;p&gt;一个CART往往过于简单无法有效地预测，因此一个更加强力的模型叫做tree ensemble。&lt;br /&gt;
&lt;img src=&quot;/images/datamining/xgboost_fig/twocart.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在上面的例子中，我们用两棵树来进行预测。我们对于每个样本的预测结果就是每棵树预测分数的和。&lt;br /&gt;
Tree ensemble一般写法为：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\begin{equation}
\hat{y_i}=\sum_{k=1}^K f_k(x_i), f_k \in \mathcal{F}
\end{equation}
&lt;/script&gt;

&lt;p&gt;其中$K$为树的棵树,每个$f$是一个在函数空间$\mathcal{F}$里面的函数，$\mathcal{F}$对应了所有CART树的集合。在XGBoost里面的目标函数也由两部分构成：损失函数+正则项，既：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\begin{equation}
Obj(\Theta)=\sum_i^n l(y_i,\hat y_i) +\sum_{k=1}^K\Omega(f_k)
\end{equation}
&lt;/script&gt;

&lt;p&gt;其中$n$为样本数目.&lt;/p&gt;

&lt;h5 id=&quot;section-5&quot;&gt;4.3 模型学习&lt;/h5&gt;
&lt;p&gt;目标函数的第一部分是训练误差，也就是大家相对比较熟悉的如平方误差, logistic loss等。而第二部分是每棵树的复杂度的和（这部分在下面介绍）。现在我们的参数可以认为是在一个函数空间里面，我们不能采用传统的如SGD之类的算法来学习我们的模型，因此我们会采用一种叫做additive training的方式。每一次保留原来的模型不变，加入一个新的函数$f$到我们的模型中。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[

\begin{equation}
\begin{split}\hat{y}_i^{(0)} &amp;= 0\\
\hat{y}_i^{(1)} &amp;= f_1(x_i) = \hat{y}_i^{(0)} + f_1(x_i)\\
\hat{y}_i^{(2)} &amp;= f_1(x_i) + f_2(x_i)= \hat{y}_i^{(1)} + f_2(x_i)\\
\hat{y}_i^{(t)} &amp;= \sum_{k=1}^t f_k(x_i)= \hat{y}_i^{(t-1)} + f_t(x_i)
\end{split}
\end{equation}
 %]]&gt;&lt;/script&gt;

&lt;p&gt;$\hat{y}_i^{(t)}$为第$t$轮的模型预测，$\hat{y}_i^{(t-1)}$保留前面$t-1$轮的模型预测，$f_t(x_i)$新加入的函数&lt;/p&gt;

&lt;p&gt;现在还剩下一个问题，我们如何选择每一轮加入什么$f$呢？答案是非常直接的，选取一个$f$来使得我们的目标函数尽量最大地降低。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[

\begin{equation}
\begin{split}\text{obj}^{(t)} &amp;= \sum_{i=1}^n l(y_i, \hat{y}_i^{(t)}) + \Omega(f_t) \\
          &amp;= \sum_{i=1}^n l(y_i, \hat{y}_i^{(t-1)} + f_t(x_i)) + \Omega(f_t) + constant
\end{split}
\end{equation}
 %]]&gt;&lt;/script&gt;

&lt;p&gt;如果$L$为平方误差的情形下，目前函数可以写成&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[

\begin{equation}
\begin{split}\text{obj}^{(t)} &amp;= \sum_{i=1}^n (y_i - (\hat{y}_i^{(t-1)} + f_t(x_i)))^2 + \sum_{i=1}^t\Omega(f_i) \\
          &amp;= \sum_{i=1}^n [2(\hat{y}_i^{(t-1)} - y_i)f_t(x_i) + f_t(x_i)^2] + \Omega(f_t) + constant
\end{split}
\end{equation}
 %]]&gt;&lt;/script&gt;

&lt;p&gt;更加一般的，对于不是平方误差的情况，我们会采用如下的泰勒展开近似来定义一个近似的目标函数，方便我们进行这一步的计算。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/datamining/xgboost_fig/model_2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;当我们把常数项移除之后，我们会发现如下一个比较统一的目标函数。这一个目标函数有一个非常明显的特点，它只依赖于每个数据点的在误差函数上的一阶导数和二阶导数.&lt;br /&gt;
误差函数为&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\begin{equation}
\sum_{i=1}^n [g_i f_t(x_i) + \frac{1}{2} h_i f_t^2(x_i)] + \Omega(f_t)
\end{equation}
&lt;/script&gt;

&lt;p&gt;一阶导数、二阶导数为&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[

\begin{equation}
\begin{split}g_i &amp;= \partial_{\hat{y}_i^{(t-1)}} \; l(y_i, \hat{y}_i^{(t-1)})\\
h_i &amp;= \partial_{\hat{y}_i^{(t-1)}}^2 \; l(y_i, \hat{y}_i^{(t-1)})
\end{split}
\end{equation}
 %]]&gt;&lt;/script&gt;

&lt;h5 id=&quot;section-6&quot;&gt;4.4 模型复杂度&lt;/h5&gt;

&lt;p&gt;先将$f$的定义做一下细化，把树拆分成结构部分$q$和叶子权重部分$w$。结构函数$q$把输入映射到叶子的索引号上面去，而$w$给定了每个索引号对应的叶子分数是什么。&lt;br /&gt;
&lt;img src=&quot;/images/datamining/xgboost_fig/model_3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;给定了如上定义之后，树的复杂度为：&lt;br /&gt;
&lt;img src=&quot;/images/datamining/xgboost_fig/model_4.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;这个复杂度包含了一棵树里面节点的个数，以及每个树叶子节点上面输出分数的$L2$模平方。当然这不是唯一的一种定义方式，只是这种方式简单并且有效。&lt;br /&gt;
### 树的结构&lt;br /&gt;
根据$q$和$w$的定义，目标函数可以改写成，其中$I$被定义为每个叶子上面样本集合$I_j = {i|q(x_i) = j} $ &lt;br /&gt;
&lt;img src=&quot;/images/datamining/xgboost_fig/model_5.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;定义$G_j = \sum&lt;em&gt;{i \in I_j} g_i \quad H_j = \sum&lt;/em&gt;{i \in I_j} h_i$，那么这个目标函数可以进一步改写成如下的形式&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[

\begin{equation}
\begin{split}
Obj^{(t)} &amp;= \sum_{j=1}^T [( \sum_{i \in I_j} g_i)w_j+\frac 1 2(\sum_{i \in I_j} h_i + \lambda)w_j^2] + \gamma T \\
&amp;= \sum_{j=1}^T [G_j w_j + \frac 1 2 (H_j + \lambda) w_j^2] + \gamma T \\
\end{split}
\end{equation}
 %]]&gt;&lt;/script&gt;

&lt;p&gt;假设我们已经知道树的结构$q$，我们可以通过这个目标函数来求解出最好的$w$，以及最好的$w$对应的目标函数最大的增益。上面的式子其实是关于$w$的一个一维二次函数的最小值的问题 ，求解既得到&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[

\begin{equation}
\begin{split}
w_j^* &amp;= - \frac {G_j} {H_j + \lambda} \\
Obj &amp;= - \frac 1 2 \sum_{j=1}^T \frac {G_j^2} {H_j + \lambda} + \gamma T
\end{split}
\end{equation}
 %]]&gt;&lt;/script&gt;

&lt;p&gt;Obj表示在某个确定的树结构下的结构分数(structure score)，这个分数可以被看做是类似gini、information gain（一般决策树评分函数）一样更加一般的对于树结构进行打分的函数。&lt;br /&gt;
&lt;img src=&quot;/images/datamining/xgboost_fig/model_6.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;section-7&quot;&gt;4.5 学习树结构&lt;/h5&gt;
&lt;p&gt;###### 4.5.1 Exact Greedy Algorithm&lt;br /&gt;
直观的方法是枚举所有的树结构，并根据上面数structure score来打分，找出最优的那棵树加入模型中，再不断重复。但暴力枚举根本不可行，所以类似于一般决策树的构建，XGBoost也是采用贪心算法，每一次尝试去对已有的叶子加入一个分割。对于一个具体的分割方案，增益计算如下：&lt;br /&gt;
&lt;img src=&quot;/images/datamining/xgboost_fig/model_7.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;对于每次树的扩展，需要枚举所有可能的分割方案，如何高效地枚举所有的分割呢？假设要枚举所有 $x&amp;lt; a$这样的条件，对于某个特定的分割$a$，要计算$a$左边和右边的导数和。&lt;br /&gt;
对于所有的$a$，首先根据需要划分的那列特征值排序，然后从左到右的扫描就可以枚举出所有分割的梯度和$G_L$和$G_R$，再用上面的公式计算每个分割方案的分数就可以了。&lt;br /&gt;
&lt;img src=&quot;/images/datamining/xgboost_fig/model_9.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;上面是针对一个特征，如果有m个特征，需要对所有参数都采取一样的操作，然后找到最好的那个特征所对应的划分。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/datamining/xgboost_fig/tree_split2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;上图是论文中原图，我理解input d(feature dimension)应该为m&lt;br /&gt;
###### 4.5.2 Approximate Algorithm &lt;br /&gt;
如果是分布式计算，则需要更好的方法。XGBoost还提出了分布式情况下的分割算法:&lt;br /&gt;
&lt;img src=&quot;/images/datamining/xgboost_fig/tree_split1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;分布式近似建树将在下面一篇博文中详细描述。&lt;/p&gt;

&lt;h6 id=&quot;sparsity-aware-split-finding&quot;&gt;4.5.3 Sparsity-aware Split Finding&lt;/h6&gt;
&lt;p&gt;实际的项目中的数据一般会存在缺失数据，因此在寻找最佳分割点时需要考虑如何处理缺失的特征，作者在论文中提出下面的算法&lt;br /&gt;
&lt;img src=&quot;/images/datamining/xgboost_fig/sparsity.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;对于特征k，在寻找split point时只对特征值为non-missing的样本上对应的特征值进行遍历，不过统计量$G$和$H$则是全部数据上的统计量。在遍历non-missing的样本$I_k$时会进行两次。按照特征值升序排列遍历时实际是将missing value划分到右叶子节点，降序遍历则是划分到左叶子节点。最后选择score最大的split point以及对应的方向作为缺失值的default directions.&lt;/p&gt;

&lt;h4 id=&quot;section-8&quot;&gt;参考资料&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;http://www.kdd.org/kdd2016/papers/files/rfp0697-chenAemb.pdf&lt;/li&gt;
  &lt;li&gt;http://www.52cs.org/?p=429&lt;/li&gt;
  &lt;li&gt;http://xgboost.readthedocs.io/en/latest/model.html&lt;/li&gt;
  &lt;li&gt;https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf&lt;/li&gt;
  &lt;li&gt;http://gaolei786.github.io/statistics/prml1.html 过拟合&lt;/li&gt;
  &lt;li&gt;http://dataunion.org/9512.html&lt;/li&gt;
  &lt;li&gt;http://www.cnblogs.com/leftnoteasy/archive/2011/03/07/random-forest-and-gbdt.html&lt;/li&gt;
  &lt;li&gt;https://statweb.stanford.edu/~jhf/ftp/trebst.pdf&lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Sun, 19 Mar 2017 00:00:00 +0800</pubDate>
	<link>http://leijun00.github.io/2017/03/xgboost-v1/</link>
	<guid isPermaLink="true">http://leijun00.github.io/2017/03/xgboost-v1/</guid>
        
        <category>GBDT</category>
        
        <category>XGBoost</category>
        
        
        <category>datamining</category>
        
      </item>
    
      <item>
        <title>DNN在CTR预估上的应用</title>
        <description>&lt;h4 id=&quot;ctr&quot;&gt;1.CTR预估&lt;/h4&gt;
&lt;p&gt;CTR预估是计算广告中最核心的算法之一，那么CTR预估是指什么呢？简单来说，CTR预估是对每次广告的点击情况做出预测，预测用户是点击还是不点击。具体定义可以参考 &lt;a href=&quot;https://en.wikipedia.org/wiki/Click-through_rate&quot;&gt;CTR&lt;/a&gt;. CTR预估和很多因素相关，比如历史点击率、广告位置、时间、用户等。CTR预估模型就是综合考虑各种因素、特征，在大量历史数据上训练得到的模型。CTR预估的训练样本一般从历史log、离线特征库获得。样本标签相对容易，用户点击标记为1，没有点击标记为0. 特征则会考虑很多，例如用户的人口学特征、广告自身特征、广告展示特征等。这些特征中会用到很多类别特征，例如用户所属职业、广告展示的IP地址等。一般对于类别特征会采样One-Hot编码，例如职业有三种：学生、白领、工人，那么会会用一个长度为3的向量分别表示他们：[1, 0, 0]、[0, 1, 0]、[0, 0, 1]. 可以这样会使得特征维度扩展很大，同时特征会非常稀疏。目前很多公司的广告特征库都是上亿级别的。&lt;/p&gt;

&lt;h4 id=&quot;dnn&quot;&gt;2.DNN&lt;/h4&gt;
&lt;p&gt;深度神经网络(DNN)近年来在图像、语音、自然语言等领域大放异彩，特别是在图像分类、语音识别、机器翻译方面DNN已经超过人，精度已经达到商业应用程度。不过，DNN在CTR预估这种场景的应用却仍在摸索中。图像、语言、自然语言领域的数据一般是连续的，局部之间存在某些结构。比如，图像的局部与其周围存在着紧密的联系；语音和文字的前后存在强相关性。但是CTR预估的数据如前面介绍，是非常离散的，特征前后之间的关系很多是我们排列的结果，并非本身是相互联系的。&lt;/p&gt;

&lt;h4 id=&quot;embeding&quot;&gt;3.Embeding&lt;/h4&gt;
&lt;p&gt;Neural Network是典型的连续值模型，而CTR预估的输入更多时候是离散特征，因此一个自然的想法就是如何将将离散特征转换为连续特征。如果你对词向量模型熟悉的话，可以发现之间的共通点。在自然语言处理(NLP)中，为了将自然语言交给机器学习中的算法来处理，通常需要首先将语言数学化，词向量就是用来将语言中的词进行数学化的一种方式。&lt;/p&gt;

&lt;p&gt;一种最简单的词向量方式是one-hot，但这么做不能很好的刻画词之间的关系(例如相似性)，另外数据规模会非常大，带来维度灾难。因此Embeding的方法被提出，基本思路是将词都映射成一个固定长度的向量(向量大小远小于one-hot编码向量大些)，向量中元素不再是只有一位是1，而是每一位都有值。将所有词向量放在一起就是一个词向量空间，这样就可以表达词之间的关系，同时达到降维的效果。&lt;/p&gt;

&lt;p&gt;既然Embeding可以将离散的词表达成连续值的词向量，那么对于CTR中的类别特征也可以使用Embeding得到连续值向量，再和其他连续值特征构成NN的输入。下图就是这种思路的表达。&lt;br /&gt;
&lt;img src=&quot;/images/deeplearning/dnn_ctr/embeding.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;因此问题的关键就是采用何种Embeding技术将离线特征转换到离线空间。&lt;/p&gt;

&lt;h6 id=&quot;fm-embeding&quot;&gt;3.1 FM Embeding&lt;/h6&gt;
&lt;p&gt;Factorization Machine是近年来在推荐、CTR预估中常用的一种算法，该算法在LR的基础上考虑交叉项，如下面公式所示：&lt;br /&gt;
&lt;img src=&quot;/images/deeplearning/dnn_ctr/fm.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;FM在后半部分的交叉项中为每个特征都分配一个特征向量V，这其实可以看作是一种Embeding的方法。Dr.Zhang在文献[1]中提出一种利用FM得到特征的embeding向量并将其组合成dense real层作为DNN的输入的模型，FNN。FNN模型的具体设计如下：&lt;br /&gt;
&lt;img src=&quot;/images/deeplearning/dnn_ctr/fnn.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Dr.Zhang在模型中做了一个假设，就是每个category field只有一个值为1，也就是每个field是个one-hot表达向量。field是指特征的种类，例如将特征occupation one-hot之后是三维向量，但这个向量都属于一个field，就是occupation。这样虽然离散化后的特征有几亿，但是category field一般是几十到几百。&lt;br /&gt;
模型得到每个特征的Embeding向量后，将特征归纳到其属于field，得到向量z，z的大小就是1+#fields * #embeding 。z是一个固定长度的向量之后再在上面加入多个隐藏层最终得到FNN模型。&lt;/p&gt;

&lt;p&gt;Dr.Zhang在FNN模型的基础上又提出了下面的新模型PNN.&lt;br /&gt;
PNN和FNN的主要不同在于除了得到z向量，还增加了一个p向量，即Product向量。Product向量由每个category field的feature vector做inner product 或则 outer product 得到，作者认为这样做有助于特征交叉。另外PNN中Embeding层不再由FM生成，可以在整个网络中训练得到。&lt;br /&gt;
&lt;img src=&quot;/images/deeplearning/dnn_ctr/pnn.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h6 id=&quot;nn-embeding&quot;&gt;3.2 NN Embeding&lt;/h6&gt;
&lt;p&gt;&lt;img src=&quot;/images/deeplearning/dnn_ctr/wide&amp;amp;deep.png&quot; alt=&quot;&quot; /&gt;&lt;br /&gt;
Google团队最近提出Wide and Deep Model。在他们的模型中，Wide Models其实就是LR模型，输入原始的特征和一些交叉组合特征；Deep Models通过Embeding层将稀疏的特征转换为稠密的特征，再使用DNN。最后将两个模型Join得到整个大模型，他们认为模型具有memorization and generalization特性。&lt;br /&gt;
Wide and Deep Model中原始特征既可以是category，也可以是continue，这样更符合一般的场景。另外Embeding层是将每个category特征分别映射到embeding size的向量，如他们在TensorFlow代码中所示：&lt;br /&gt;
&lt;code&gt;
deep_columns = [
  tf.contrib.layers.embedding_column(workclass, dimension=8),
  tf.contrib.layers.embedding_column(education, dimension=8),
  tf.contrib.layers.embedding_column(gender, dimension=8),
  tf.contrib.layers.embedding_column(relationship, dimension=8),
  tf.contrib.layers.embedding_column(native_country, dimension=8),
  tf.contrib.layers.embedding_column(occupation, dimension=8),
  age, education_num, capital_gain, capital_loss, hours_per_week]
&lt;/code&gt;&lt;/p&gt;

&lt;h4 id=&quot;section&quot;&gt;4.结合图像&lt;/h4&gt;
&lt;p&gt;目前很多在线广告都是图片形式的，文献[4]提出将图像也做为特征的输入。这样原始特征就分为两类，图像部分使用CNN，非图像部分使用NN处理。&lt;br /&gt;
其实这篇文章并没有太多新颖的方法，只能说多了一种特征。对于非图像特征，作者直接使用全连接神经网络，并没有使用Embeding。&lt;br /&gt;
&lt;img src=&quot;/images/deeplearning/dnn_ctr/conv_ctr.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;cnn&quot;&gt;5.CNN&lt;/h4&gt;
&lt;p&gt;CNN用于提取局部特征，在图像、NLP都取得不错的效果，如果在CTR预估中使用却是个难题。我认为最大的困难时如何构建对一个样本构建如图像那样的矩阵，能够具有局部联系和结构。如果不能构造这样的矩阵，使用CNN是没有什么意思的。&lt;br /&gt;
文献[5]是发表在CIKM2015的一篇短文，文章提出对使用CNN来进行CTR预估进行了尝试。&lt;br /&gt;
一条广告展示(single ad impression)包括：element = (user; query; ad, impression time, site category, device type, etc)&lt;br /&gt;
用户是否点击一个广告与用户的历史ad impression有关。这样，一个样本将会是(s, label) ，s由多条l组成(数目不定)&lt;br /&gt;
&lt;img src=&quot;/images/deeplearning/dnn_ctr/s_matrix.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;作者提出CCPM模型处理这样的数据。每个样本有n个element，对每个element使用embeding 得到定长为d的向量$e_i\in R^d$，再构造成一个矩阵$s\in R^{d* n}$，得到s矩阵之后就可以套用CNN，后面的其实没有太多创新点。&lt;br /&gt;
&lt;img src=&quot;/images/deeplearning/dnn_ctr/ccpm.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;rnn&quot;&gt;6.RNN&lt;/h4&gt;
&lt;p&gt;考虑搜索场景下的CTR预估，如果考虑历史信息，如可以将一个用户的历史ad impression构成一个时间序列。RNN非常适合时间序列的场景，如语言建模等。&lt;a href=&quot;http://dl.acm.org/citation.cfm?id=2893873.2894086&quot;&gt;这篇&lt;/a&gt; 发表在AAAI2014将RNN模型引入CTR预估。作者首先在数据集上验证了用户的点击行为与之前的ad impression历史有关联：&lt;br /&gt;
* 如果用户在之前的impression很快离开广告页面，那么将会在接下来一段时间内不会点击类似的广告&lt;br /&gt;
* 如果用户最近有过与广告相关的查询，那么接下来点击相关广告的可能性会大幅提升&lt;br /&gt;
* 前面的两种行为还可能随着间隔时间的增加而不是那么相关&lt;/p&gt;

&lt;p&gt;当前关联不止这些，而且人工难以刻画，需要模型来自动提取。RNN模型对此类问题非常适用，作者的主要工作是将数据集构造成适合RNN的输入（即对用户的历史ad impression根据时间排序），对模型本身并没有改进。&lt;br /&gt;
&lt;img src=&quot;/images/deeplearning/dnn_ctr/rnn_ctr.png&quot; alt=&quot;&quot; /&gt; &lt;br /&gt;
#### 参考文献&lt;br /&gt;
1. Deep Learning over Multi-field Categorical Data – A Case Study on User Response Prediction&lt;br /&gt;
2. Product-based Neural Networks for User Response Prediction&lt;br /&gt;
3. Wide &amp;amp; Deep Learning for Recommender Systems &lt;br /&gt;
4. Deep CTR Prediction in Display Advertising&lt;br /&gt;
5. A Convolutional Click Prediction Model&lt;br /&gt;
6. http://www.52cs.org/?p=1046&lt;br /&gt;
7. http://techshow.ctrip.com/archives/1149.html&lt;br /&gt;
8. http://tech.meituan.com/deep-understanding-of-ffm-principles-and-practices.html &lt;br /&gt;
9. Sequential Click Prediction for Sponsored Search with Recurrent Neural Networks&lt;/p&gt;

</description>
        <pubDate>Thu, 16 Mar 2017 00:00:00 +0800</pubDate>
	<link>http://leijun00.github.io/2017/03/dnn-for-ctr/</link>
	<guid isPermaLink="true">http://leijun00.github.io/2017/03/dnn-for-ctr/</guid>
        
        <category>DNN</category>
        
        <category>CTR</category>
        
        
        <category>deeplearning</category>
        
      </item>
    
      <item>
        <title>CNN在NLP的应用--文本分类</title>
        <description>&lt;p&gt;刚接触深度学习时知道CNN一般用于计算机视觉，RNN等一般用于自然语言相关。CNN目前在CV领域独领风骚，自然就有想法将CNN迁移到NLP中。但是NLP与CV不太一样，NLP有语言内存的结构，所以最开始CNN在NLP领域的应用在文本分类。相比于具体的句法分析、语义分析的应用，文本分类不需要精准分析。本文主要介绍最近学习到几个算法，并用mxnet进行了实现，如有错误请大家指出。&lt;/p&gt;

&lt;h4 id=&quot;convolutional-neural-networks-for-sentence-classification&quot;&gt;1 Convolutional Neural Networks for Sentence Classification&lt;/h4&gt;
&lt;p&gt;##### 1.1 原理&lt;br /&gt;
&lt;img src=&quot;/images/deeplearning/cnn_nlp/model1.png&quot; alt=&quot;&quot; /&gt;&lt;br /&gt;
CNN的输入是矩阵形式，因此首先是构造矩阵。句子有词构成，DL中一般使用词向量，再对句子的长度设置一个固定值（可以是最大长度），那么就可以构造一个矩阵，这样就可以应用CNN了。这篇论文就是这样的思想，如上图所示。&lt;/p&gt;

&lt;h6 id=&quot;section&quot;&gt;输入层&lt;/h6&gt;
&lt;p&gt;句子长度为$n$，词向量的维度为$k$，那么矩阵就是$n*k$。具体的，词向量可以是静态的或者动态的。静态指词向量提取利用word2vec等得到，动态指词向量是在模型整体训练过程中得到。&lt;/p&gt;

&lt;h6 id=&quot;section-1&quot;&gt;卷积层&lt;/h6&gt;
&lt;p&gt;一个卷积层的kernel大小为$h&lt;em&gt;k$，$k$为输入层词向量的维度，那么$h$为窗口内词的数目。这样可以看做为N-Gram的变种。如此一个卷积操作可以得到一个$(n-h+1)&lt;/em&gt;1$的feature map。多个卷积操作就可以得到多个这样的feature map&lt;/p&gt;

&lt;h6 id=&quot;section-2&quot;&gt;池化层&lt;/h6&gt;
&lt;p&gt;这里面的池化层比较简单，就是一个Max-over-time Pooling，从前面的1维的feature map中取最大值。&lt;a href=&quot;http://blog.csdn.net/malefactor/article/details/51078135#0-tsina-1-38411-397232819ff9a47a7b7e80a40613cfe1&quot;&gt;这篇文章&lt;/a&gt;中给出了NLP中CNN的常用Pooling方法。最终将会得到1维的size=m的向量(m=卷积的数目)&lt;/p&gt;

&lt;h6 id=&quot;section-3&quot;&gt;全连接+输出层&lt;/h6&gt;
&lt;p&gt;模型的输出层就是全连接+Softmax。可以加上通用的Dropout和正则的方法来优化&lt;/p&gt;

&lt;h5 id=&quot;section-4&quot;&gt;1.2 实现&lt;/h5&gt;
&lt;p&gt;&lt;a href=&quot;http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/&quot;&gt;这篇文章&lt;/a&gt;用TensorFlow实现了这个模型，&lt;a href=&quot;https://github.com/dennybritz/cnn-text-classification-tf&quot;&gt;代码&lt;/a&gt;。我参考这个代码用mxnet实现了，&lt;a href=&quot;https://github.com/yxzf/cnn-text-classification-mx&quot;&gt;代码&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;character-level-convolutional-networks-for-text-classification&quot;&gt;2 Character-level Convolutional Networks for Text Classification&lt;/h4&gt;
&lt;p&gt;##### 2.1 原理&lt;br /&gt;
图像的基本都要是像素单元，那么在语言中基本的单元应该是字符。CNN在图像中有效就是从原始特征不断向上提取高阶特征，那么在NLP中可以从字符构造矩阵，再应用CNN来做。这篇论文就是这个思路。&lt;br /&gt;
&lt;img src=&quot;/images/deeplearning/cnn_nlp/model2.png&quot; alt=&quot;&quot; /&gt;&lt;br /&gt;
###### 输入层&lt;br /&gt;
一个句子构造一个矩阵，句子由字符构成。设定一个句子的字符数目$n$(论文中$n=1014$)。每个字符是一个字符向量，这个字符向量可以是one-hot向量，也可以是一个char embeding. 设字符向量为$k$，那么矩阵为$n*k$。&lt;br /&gt;
###### 卷积层&lt;br /&gt;
这里的卷积层就是套用在图像领域的， 论文中给出具体的设置&lt;br /&gt;
&lt;img src=&quot;/images/deeplearning/cnn_nlp/model2_conv.png&quot; alt=&quot;&quot; /&gt;&lt;br /&gt;
###### 全连接+输出层&lt;br /&gt;
全连接层的具体设置如下&lt;br /&gt;
![/images/deeplearning/cnn_nlp/model2_fc.png]&lt;/p&gt;

&lt;h5 id=&quot;section-5&quot;&gt;2.2 实现&lt;/h5&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/scharmchi/char-level-cnn-tf&quot;&gt;代码&lt;/a&gt;给出了TensorFlow的实现，我用mxnet实现的&lt;a href=&quot;https://github.com/yxzf/char-level-cnn-mx&quot;&gt;代码&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;character-aware-neural-language-models&quot;&gt;3 Character-Aware Neural Language Models&lt;/h4&gt;
&lt;p&gt;##### 3.1 原理&lt;br /&gt;
这篇文章提出一种CNN+RNN结合的模型。CNN部分，每个单词由character组成，如果对character构造embeding向量，则可以对单词构造矩阵作为CNN的输入。CNN的输出为词向量，作为RNN的输入，RNN的输出则是以整个词为单位。&lt;br /&gt;
&lt;img src=&quot;/images/deeplearning/cnn_nlp/char-cnn-rnn.png&quot; alt=&quot;&quot; /&gt;&lt;br /&gt;
###### 3.1.1 CNN层&lt;br /&gt;
&lt;strong&gt;输入层&lt;/strong&gt;: 一个句子(sentence)是一个输入样本，句子由词(word)构成，词由字符(character)组成。每个字符学习一个embeding字符向量，设字符向量长度为$k$，那么一个word(长度为$w$)就可以构造成一个矩阵C($k*w$)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;卷积层&lt;/strong&gt;:&lt;br /&gt;
对这个矩阵C使用多个卷积层，每个卷积层的kernel大小为($k&lt;em&gt;n$)。卷积层可以看作是character的n-gram，那么每个卷积操作后得到的矩阵为($1&lt;/em&gt;(w-n+1)$)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;池化层&lt;/strong&gt;:&lt;br /&gt;
池化层仍是max-pooling，挑选出($w-n+1$)长度向量中的最大值，将所有池化层的结果拼接就可以得到定长的向量$p$，$p$的长度为所有卷积层的数目&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Highway层&lt;/strong&gt;:&lt;br /&gt;
&lt;a href=&quot;https://arxiv.org/pdf/1505.00387.pdf&quot;&gt;Highway层&lt;/a&gt;是最近刚提出的一种结构，借鉴LSTM的gate概念。$x$为该层的输入，那么首先计算一个非线性转换$T(x)$，$T(x)\in [0, 1]$($T$一般为sigmod)。除了$T(x)$，还有另外一个非线性转换$H(x)$，最终的输出为$y = T(x) * H(x) + (1 - T(x)) * x$。从公式来看，$T(x)$充当了gate，如果$T(x)=1$，那么输出同传统的非线性层一样，输出是$H(x)$，如果$T(x)=0$，则直接输出输入$x$。作者认为Highway层的引入避免了梯度的快速消失，这种特性可以构建更深的网络。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;输出层&lt;/strong&gt;:&lt;br /&gt;
每个单词将得到一个词向量，与一般的词向量获取不同，这里的词向量是DNN在character embeding上得到的。&lt;/p&gt;

&lt;h6 id=&quot;rnn&quot;&gt;3.1.2 RNN层&lt;/h6&gt;
&lt;p&gt;&lt;strong&gt;输入层&lt;/strong&gt;:&lt;br /&gt;
将一个句子的每个CNN输出作为词向量，整个句子就是RNN的输入层&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;隐藏层&lt;/strong&gt;:&lt;br /&gt;
一般使用LSTM，也可以是GRU&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;输出层&lt;/strong&gt;:&lt;br /&gt;
输出层以word为单位，而不是character. &lt;/p&gt;

&lt;h5 id=&quot;section-6&quot;&gt;3.2实现&lt;/h5&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/carpedm20/lstm-char-cnn-tensorflow&quot;&gt;代码&lt;/a&gt;给出了TensorFlow的实现，我的MXNet实现见&lt;a href=&quot;https://github.com/yxzf/lstm-char-cnn-mx&quot;&gt;代码&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;section-7&quot;&gt;参考资料&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;https://github.com/Lasagne/Lasagne/blob/highway_example/examples/Highway%20Networks.ipynb&lt;/li&gt;
  &lt;li&gt;http://www.jeyzhang.com/cnn-apply-on-modelling-sentence.html&lt;/li&gt;
  &lt;li&gt;http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/&lt;/li&gt;
  &lt;li&gt;http://www.wtoutiao.com/p/H08qKy.html&lt;/li&gt;
  &lt;li&gt;http://karpathy.github.io/2015/05/21/rnn-effectiveness/&lt;/li&gt;
  &lt;li&gt;https://papers.nips.cc/paper/5782-character-level-convolutional-networks-for-text-classification.pdf&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Sun, 12 Mar 2017 00:00:00 +0800</pubDate>
	<link>http://leijun00.github.io/2017/03/cnn-for-nlp-in-sentence-classification/</link>
	<guid isPermaLink="true">http://leijun00.github.io/2017/03/cnn-for-nlp-in-sentence-classification/</guid>
        
        <category>CNN</category>
        
        <category>NLP</category>
        
        
        <category>deeplearning</category>
        
      </item>
    
  </channel>
</rss>
