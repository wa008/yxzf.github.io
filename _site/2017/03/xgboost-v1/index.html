<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <title>XGBoost解读(1)--原理 • YXZF's  Blog</title>
    <meta name="description" content="1 集成学习
科学研究中，有种方法叫做组合，甚是强大，小硕们毕业基本靠它了。将别人的方法一起组合起来然后搞成一个集成的算法，集百家之长，效果一般不会差。其实也不能怪小硕们，大牛们也有这么做的，只是大牛们做的比较漂亮。

">
    <meta name="keywords" content="GBDT, XGBoost">
    
    
    	<!-- Twitter Cards -->
	<meta name="twitter:title" content="XGBoost解读(1)--原理">
	<meta name="twitter:description" content="1 集成学习
科学研究中，有种方法叫做组合，甚是强大，小硕们毕业基本靠它了。将别人的方法一起组合起来然后搞成一个集成的算法，集百家之长，效果一般不会差。其实也不能怪小硕们，大牛们也有这么做的，只是大牛们做的比较漂亮。

">
	
	
	
	<meta name="twitter:card" content="summary">
	<meta name="twitter:image" content="/images/logo.jpg">
	
	<!-- Open Graph -->
	<meta property="og:locale" content="">
	<meta property="og:type" content="article">
	<meta property="og:title" content="XGBoost解读(1)--原理">
	<meta property="og:description" content="1 集成学习
科学研究中，有种方法叫做组合，甚是强大，小硕们毕业基本靠它了。将别人的方法一起组合起来然后搞成一个集成的算法，集百家之长，效果一般不会差。其实也不能怪小硕们，大牛们也有这么做的，只是大牛们做的比较漂亮。

">
	<meta property="og:url" content="/2017/03/xgboost-v1/">
	<meta property="og:site_name" content="YXZF's  Blog">
    

    <link rel="canonical" href="/2017/03/xgboost-v1/">

    <link href="/atom.xml" type="application/atom+xml" rel="alternate" title="YXZF's  Blog Atom Feed">
    <link href="/sitemap.xml" type="application/xml" rel="sitemap" title="Sitemap">

    <meta name="HandheldFriendly" content="True">
    <meta name="MobileOptimized" content="320">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="cleartype" content="on">

    <style>
    .sliding-menu-content {
      top: 0;
      right: 0;
      text-align: center;
      visibility: hidden;
      height: 100%;
      width: 100%;
      -webkit-transform: translateX(100%);
      -moz-transform: translateX(100%);
      -ms-transform: translateX(100%);
      -o-transform: translateX(100%);
      transform: translateX(100%);
    }
    </style>

    <link rel="stylesheet" href="/css/main.css">
    <!-- HTML5 Shiv and Media Query Support for IE -->
    <!--[if lt IE 9]>
      <script src="/js/vendor/html5shiv.min.js"></script>
      <script src="/js/vendor/respond.min.js"></script>
    <![endif]-->

  </head>

  <body>
    <header id="masthead">
  <div class="inner-wrap">
    <a href="/" class="site-title">YXZF's  Blog</a>
    <nav role="navigation" class="menu top-menu">
        <ul class="menu-item">
	<li class="home"><a href="/">YXZF's  Blog</a></li>
	
    
        
    
    <li><a href="/" >主页</a></li>
  
    
        
    
    <li><a href="/datamining/" >数据挖掘</a></li>
  
    
        
    
    <li><a href="/deeplearning/" >深度学习</a></li>
  
    
        
    
    <li><a href="/development/" >开发</a></li>
  
    
        
    
    <li><a href="/math/" >数学</a></li>
  
    
        
    
    <li><a href="/about/" >关于</a></li>
  
</ul>
    </nav>
  </div><!-- /.inner-wrap -->
</header><!-- /.masthead -->
    <nav role="navigation" class="js-menu sliding-menu-content">
    <ul class="menu-item">
        <li>
      
        
      
            
            <a href="/" class="title">主页</a>
            
        </li><li>
      
        
      
            
            <a href="/datamining/" class="title">数据挖掘</a>
            
        </li><li>
      
        
      
            
            <a href="/deeplearning/" class="title">深度学习</a>
            
        </li><li>
      
        
      
            
            <a href="/development/" class="title">开发</a>
            
        </li><li>
      
        
      
            
            <a href="/math/" class="title">数学</a>
            
        </li><li>
      
        
      
            
            <a href="/about/" class="title">关于</a>
            
        </li>
    </ul>
</nav>
<button type="button" class="js-menu-trigger sliding-menu-button menulines-button x2" role="button" aria-label="Toggle Navigation">
    <span class="menulines"></span>
</button>

<div class="js-menu-screen menu-screen"></div>


    <div id="page-wrapper">
      <!--[if lt IE 9]><div class="upgrade notice-danger"><strong>Your browser is quite old!</strong> Why not <a href="http://whatbrowser.org/">upgrade to a newer one</a> to better enjoy this site?</div><![endif]-->

      <div id="main" role="main">
    <article class="wrap" itemscope itemtype="http://schema.org/Article">
        
        <div class="page-title">
            <h1>XGBoost解读(1)--原理</h1>
        </div>
        <div class="inner-wrap">
            
            <div id="content" class="page-content" itemprop="articleBody">
                <h4 id="section">1 集成学习</h4>
<p>科学研究中，有种方法叫做组合，甚是强大，小硕们毕业基本靠它了。将别人的方法一起组合起来然后搞成一个集成的算法，集百家之长，效果一般不会差。其实也不能怪小硕们，大牛们也有这么做的，只是大牛们做的比较漂亮。</p>

<p>在PAC学习框架下（Probably Approximately Correct）, Kearns和Valiant指出，若存在一个多项式级的学习算法来识别一组概念，并且识别正确率很高，那么这组概念是强可学习的；而如果学习算法识别一组概念的正确率仅比随机猜测略好，那么这组概念是弱可学习的。Schapire证明了弱学习算法与强学习算法的等价性问题，这样在学习概念时，只要找到一个比随机猜测略好的弱学习算法，就可以将其提升为强学习算法，而不必直接去找通常情况下很难获得的强学习算法。这为集成学习提供了理论支持。</p>

<p>在众多单模型中（与集成模型相对应），决策树这种算法有着很多良好的特性，比如说训练时间复杂度较低，预测的过程比较快速，模型可解释强等。但是同时，单决策树又有一些不好的地方，比如说容易over-fitting，虽然有一些方法，如剪枝可以减少这种情况，但是还是不够的。</p>

<p>集成学习还是有很多方法的，在这里只介绍两种普遍做法：bagging集成和boosting集成。bagging集成，例如随机森林，对样本随机抽样建立很多树，每棵树之间没有关联，这些树组成森林，构成随机森林模型。boosting集成后一个模型是对前一个模型产生误差的矫正。gradient boost更具体，是指每个新模型的引入是为了减少上个模型的残差(residual)，而为了消除残差，我们可以在残差减少的梯度(Gradient)方向上建立一个新的模型。如果基础模型是决策树，那么这样的模型就被称为Gradient Boost Decision Tree（GBDT）</p>

<h4 id="xgboost">2 XGBoost</h4>
<p>GBDT可以看做是个框架，最早的一种实现方法由Friedman 在论文GREEDY FUNCTION APPROXIMATION: A GRADIENT BOOSTING MACHINE 。xgboost（eXtreme Gradient Boosting）是最近提出的一个新的GBDT实现，由陈天奇提出，在Kaggle、KDD Cup等数据挖掘比赛中大方异彩，成为冠军队伍的标配，另外很多大公司，如腾讯、阿里、美团已在公司里面部署。XGBoost有如下优点:</p>

<ul>
  <li>显示的把树模型复杂度作为正则项加到优化目标中。</li>
  <li>公式推导中用到了二阶导数，用了二阶泰勒展开。</li>
  <li>实现了分裂点寻找近似算法。</li>
  <li>利用了特征的稀疏性。</li>
  <li>数据事先排序并且以block形式存储，有利于并行计算。</li>
  <li>基于分布式通信框架rabit，可以运行在MPI和yarn上。</li>
  <li>实现做了面向体系结构的优化，针对cache和内存做了性能优化。</li>
</ul>

<h4 id="section-1">3 监督学习要素</h4>
<p>XGBoost应用于监督学习问题，使用训练数据(有很多特征)$x_i$来预测目标$y_i$。 监督学习的三要素：模型、参数和目标函数</p>

<h5 id="section-2">3.1 模型</h5>
<p>模型指给定输入$x_i$如何去预测输出$y_i$。我们比较常见的模型如线性模型（包括线性回归和logistic regression）采用了线性叠加的方式进行预测$\hat{y}_i=\sum_j w_j x_{ij}$. 其实这里的预测$y$可以有不同的解释，比如我们可以用它来作为回归目标的输出，或者进行sigmoid 变换得到概率，或者作为排序的指标等。而一个线性模型根据$y$的解释不同（以及设计对应的目标函数）用到回归，分类或排序等场景。</p>

<h5 id="section-3">3.2 参数</h5>
<p>参数指我们需要学习的东西，在线性模型中，参数指我们的线性系数$w$。</p>

<h5 id="section-4">3.3 目标函数</h5>
<p>模型和参数本身指定了给定输入我们如何做预测，但是没有告诉我们如何去寻找一个比较好的参数，这个时候就需要目标函数登场了。一般的目标函数包含下面两项：<br />
<img src="/images/datamining/xgboost_fig/reg_1.png" alt="" /></p>

<p>其中$L$为训练损失函数，$\Sigma$是正则项，惩罚模型复杂度。<br />
常见的损失函数，对于回归问题，有损失函数L为最小平方误差: $L(y, \hat{y}) = (y - \hat{y})^2$，对于二分类，损失函数L为logit loss $L(y, \hat{y})=-log\,\sigma(y, \hat{y})$<br />
为什么加入正则项？因为模型是在训练集上训练，但是实际应用时时另一份数据集，一般为测试集。那么模型在训练集上表现优异，不代表在测试集上会好。越是在训练集上模型越是复杂，则模型过拟合会比较严重。</p>

<p><img src="/images/datamining/xgboost_fig/reg_3.png" alt="" /><br />
<img src="/images/datamining/xgboost_fig/reg_2.png" alt="" /></p>

<p>奥卡姆剃刀原则：在所有可能选择的模型中，能够很好解释已知数据，并且十分简单的模型才是最好的模型。总而言之，加入正则项是为了提高模型的泛化能力，既在未知数据集上同样表现良好。<br />
常见的正则化项有L1正则和L2正则。L1正则项是参数向量的L1范数，L2正则项是参数向量的L2范数</p>

<h4 id="boosted-tree">4  Boosted Tree</h4>
<p>##### 4.1 基学习器CART<br />
Boosted tree 最基本的组成部分叫做回归树(regression tree)，也叫做CART。<br />
<img src="/images/datamining/xgboost_fig/cart.png" alt="" /></p>

<p>上面就是一个CART的例子。CART会把输入根据输入的属性分配到各个叶子节点，而每个叶子节点上面都会对应一个实数分数，你可以把叶子的分数理解为有多可能这个人喜欢电脑游戏。<br />
&gt; CART树模型的参数是什么？树的参数一般为树的结构、树的叶子节点、叶子节点的值等</p>

<h5 id="tree-ensemble">4.2 Tree Ensemble</h5>
<p>一个CART往往过于简单无法有效地预测，因此一个更加强力的模型叫做tree ensemble。<br />
<img src="/images/datamining/xgboost_fig/twocart.png" alt="" /></p>

<p>在上面的例子中，我们用两棵树来进行预测。我们对于每个样本的预测结果就是每棵树预测分数的和。<br />
Tree ensemble一般写法为：</p>

<script type="math/tex; mode=display">
\begin{equation}
\hat{y_i}=\sum_{k=1}^K f_k(x_i), f_k \in \mathcal{F}
\end{equation}
</script>

<p>其中$K$为树的棵树,每个$f$是一个在函数空间$\mathcal{F}$里面的函数，$\mathcal{F}$对应了所有CART树的集合。在XGBoost里面的目标函数也由两部分构成：损失函数+正则项，既：</p>

<script type="math/tex; mode=display">
\begin{equation}
Obj(\Theta)=\sum_i^n l(y_i,\hat y_i) +\sum_{k=1}^K\Omega(f_k)
\end{equation}
</script>

<p>其中$n$为样本数目.</p>

<h5 id="section-5">4.3 模型学习</h5>
<p>目标函数的第一部分是训练误差，也就是大家相对比较熟悉的如平方误差, logistic loss等。而第二部分是每棵树的复杂度的和（这部分在下面介绍）。现在我们的参数可以认为是在一个函数空间里面，我们不能采用传统的如SGD之类的算法来学习我们的模型，因此我们会采用一种叫做additive training的方式。每一次保留原来的模型不变，加入一个新的函数$f$到我们的模型中。</p>

<script type="math/tex; mode=display">% <![CDATA[

\begin{equation}
\begin{split}\hat{y}_i^{(0)} &= 0\\
\hat{y}_i^{(1)} &= f_1(x_i) = \hat{y}_i^{(0)} + f_1(x_i)\\
\hat{y}_i^{(2)} &= f_1(x_i) + f_2(x_i)= \hat{y}_i^{(1)} + f_2(x_i)\\
\hat{y}_i^{(t)} &= \sum_{k=1}^t f_k(x_i)= \hat{y}_i^{(t-1)} + f_t(x_i)
\end{split}
\end{equation}
 %]]></script>

<p>$\hat{y}_i^{(t)}$为第$t$轮的模型预测，$\hat{y}_i^{(t-1)}$保留前面$t-1$轮的模型预测，$f_t(x_i)$新加入的函数</p>

<p>现在还剩下一个问题，我们如何选择每一轮加入什么$f$呢？答案是非常直接的，选取一个$f$来使得我们的目标函数尽量最大地降低。</p>

<script type="math/tex; mode=display">% <![CDATA[

\begin{equation}
\begin{split}\text{obj}^{(t)} &= \sum_{i=1}^n l(y_i, \hat{y}_i^{(t)}) + \Omega(f_t) \\
          &= \sum_{i=1}^n l(y_i, \hat{y}_i^{(t-1)} + f_t(x_i)) + \Omega(f_t) + constant
\end{split}
\end{equation}
 %]]></script>

<p>如果$L$为平方误差的情形下，目前函数可以写成</p>

<script type="math/tex; mode=display">% <![CDATA[

\begin{equation}
\begin{split}\text{obj}^{(t)} &= \sum_{i=1}^n (y_i - (\hat{y}_i^{(t-1)} + f_t(x_i)))^2 + \sum_{i=1}^t\Omega(f_i) \\
          &= \sum_{i=1}^n [2(\hat{y}_i^{(t-1)} - y_i)f_t(x_i) + f_t(x_i)^2] + \Omega(f_t) + constant
\end{split}
\end{equation}
 %]]></script>

<p>更加一般的，对于不是平方误差的情况，我们会采用如下的泰勒展开近似来定义一个近似的目标函数，方便我们进行这一步的计算。</p>

<p><img src="/images/datamining/xgboost_fig/model_2.png" alt="" /></p>

<p>当我们把常数项移除之后，我们会发现如下一个比较统一的目标函数。这一个目标函数有一个非常明显的特点，它只依赖于每个数据点的在误差函数上的一阶导数和二阶导数.<br />
误差函数为</p>

<script type="math/tex; mode=display">
\begin{equation}
\sum_{i=1}^n [g_i f_t(x_i) + \frac{1}{2} h_i f_t^2(x_i)] + \Omega(f_t)
\end{equation}
</script>

<p>一阶导数、二阶导数为</p>

<script type="math/tex; mode=display">% <![CDATA[

\begin{equation}
\begin{split}g_i &= \partial_{\hat{y}_i^{(t-1)}} \; l(y_i, \hat{y}_i^{(t-1)})\\
h_i &= \partial_{\hat{y}_i^{(t-1)}}^2 \; l(y_i, \hat{y}_i^{(t-1)})
\end{split}
\end{equation}
 %]]></script>

<h5 id="section-6">4.4 模型复杂度</h5>

<p>先将$f$的定义做一下细化，把树拆分成结构部分$q$和叶子权重部分$w$。结构函数$q$把输入映射到叶子的索引号上面去，而$w$给定了每个索引号对应的叶子分数是什么。<br />
<img src="/images/datamining/xgboost_fig/model_3.png" alt="" /></p>

<p>给定了如上定义之后，树的复杂度为：<br />
<img src="/images/datamining/xgboost_fig/model_4.png" alt="" /></p>

<p>这个复杂度包含了一棵树里面节点的个数，以及每个树叶子节点上面输出分数的$L2$模平方。当然这不是唯一的一种定义方式，只是这种方式简单并且有效。<br />
### 树的结构<br />
根据$q$和$w$的定义，目标函数可以改写成，其中$I$被定义为每个叶子上面样本集合$I_j = {i|q(x_i) = j} $ <br />
<img src="/images/datamining/xgboost_fig/model_5.png" alt="" /></p>

<p>定义$G_j = \sum<em>{i \in I_j} g_i \quad H_j = \sum</em>{i \in I_j} h_i$，那么这个目标函数可以进一步改写成如下的形式</p>

<script type="math/tex; mode=display">% <![CDATA[

\begin{equation}
\begin{split}
Obj^{(t)} &= \sum_{j=1}^T [( \sum_{i \in I_j} g_i)w_j+\frac 1 2(\sum_{i \in I_j} h_i + \lambda)w_j^2] + \gamma T \\
&= \sum_{j=1}^T [G_j w_j + \frac 1 2 (H_j + \lambda) w_j^2] + \gamma T \\
\end{split}
\end{equation}
 %]]></script>

<p>假设我们已经知道树的结构$q$，我们可以通过这个目标函数来求解出最好的$w$，以及最好的$w$对应的目标函数最大的增益。上面的式子其实是关于$w$的一个一维二次函数的最小值的问题 ，求解既得到</p>

<script type="math/tex; mode=display">% <![CDATA[

\begin{equation}
\begin{split}
w_j^* &= - \frac {G_j} {H_j + \lambda} \\
Obj &= - \frac 1 2 \sum_{j=1}^T \frac {G_j^2} {H_j + \lambda} + \gamma T
\end{split}
\end{equation}
 %]]></script>

<p>Obj表示在某个确定的树结构下的结构分数(structure score)，这个分数可以被看做是类似gini、information gain（一般决策树评分函数）一样更加一般的对于树结构进行打分的函数。<br />
<img src="/images/datamining/xgboost_fig/model_6.png" alt="" /></p>

<h5 id="section-7">4.5 学习树结构</h5>
<p>###### 4.5.1 Exact Greedy Algorithm<br />
直观的方法是枚举所有的树结构，并根据上面数structure score来打分，找出最优的那棵树加入模型中，再不断重复。但暴力枚举根本不可行，所以类似于一般决策树的构建，XGBoost也是采用贪心算法，每一次尝试去对已有的叶子加入一个分割。对于一个具体的分割方案，增益计算如下：<br />
<img src="/images/datamining/xgboost_fig/model_7.png" alt="" /></p>

<p>对于每次树的扩展，需要枚举所有可能的分割方案，如何高效地枚举所有的分割呢？假设要枚举所有 $x&lt; a$这样的条件，对于某个特定的分割$a$，要计算$a$左边和右边的导数和。<br />
对于所有的$a$，首先根据需要划分的那列特征值排序，然后从左到右的扫描就可以枚举出所有分割的梯度和$G_L$和$G_R$，再用上面的公式计算每个分割方案的分数就可以了。<br />
<img src="/images/datamining/xgboost_fig/model_9.png" alt="" /></p>

<p>上面是针对一个特征，如果有m个特征，需要对所有参数都采取一样的操作，然后找到最好的那个特征所对应的划分。</p>

<p><img src="/images/datamining/xgboost_fig/tree_split2.png" alt="" /></p>

<blockquote>
  <p>上图是论文中原图，我理解input d(feature dimension)应该为m<br />
###### 4.5.2 Approximate Algorithm <br />
如果是分布式计算，则需要更好的方法。XGBoost还提出了分布式情况下的分割算法:<br />
<img src="/images/datamining/xgboost_fig/tree_split1.png" alt="" /></p>
</blockquote>

<p>分布式近似建树将在下面一篇博文中详细描述。</p>

<h6 id="sparsity-aware-split-finding">4.5.3 Sparsity-aware Split Finding</h6>
<p>实际的项目中的数据一般会存在缺失数据，因此在寻找最佳分割点时需要考虑如何处理缺失的特征，作者在论文中提出下面的算法<br />
<img src="/images/datamining/xgboost_fig/sparsity.png" alt="" /></p>

<p>对于特征k，在寻找split point时只对特征值为non-missing的样本上对应的特征值进行遍历，不过统计量$G$和$H$则是全部数据上的统计量。在遍历non-missing的样本$I_k$时会进行两次。按照特征值升序排列遍历时实际是将missing value划分到右叶子节点，降序遍历则是划分到左叶子节点。最后选择score最大的split point以及对应的方向作为缺失值的default directions.</p>

<h4 id="section-8">参考资料</h4>
<ol>
  <li>http://www.kdd.org/kdd2016/papers/files/rfp0697-chenAemb.pdf</li>
  <li>http://www.52cs.org/?p=429</li>
  <li>http://xgboost.readthedocs.io/en/latest/model.html</li>
  <li>https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf</li>
  <li>http://gaolei786.github.io/statistics/prml1.html 过拟合</li>
  <li>http://dataunion.org/9512.html</li>
  <li>http://www.cnblogs.com/leftnoteasy/archive/2011/03/07/random-forest-and-gbdt.html</li>
  <li>https://statweb.stanford.edu/~jhf/ftp/trebst.pdf</li>
</ol>


                <hr />
                <footer class="page-footer">
                    


<div class="author-image">
	<img src="/images/logo.jpg" alt="沈成光">
</div><!-- ./author-image -->
<div class="author-content">
	<h3 class="author-name" >Written by <span itemprop="author">沈成光</span></h3>
	<p class="author-bio"></p>
</div><!-- ./author-content -->
                    <div class="inline-btn">
    <a class="btn-social twitter" href="https://twitter.com/intent/tweet?text=XGBoost解读(1)--原理&amp;url=/2017/03/xgboost-v1/&amp;via=" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i> Share on Twitter</a> 
<!-- <a class="btn-social twitter" href="http://service.weibo.com/share/share.php?title=XGBoost解读(1)--原理&url=/2017/03/xgboost-v1/&changweibo=yes&ralateUid=1278841597" target='_newtab' type="submit" class="submit">分享到微博</a> -->
    <a class="btn-social facebook" href="https://www.facebook.com/sharer/sharer.php?u=/2017/03/xgboost-v1/" target="_blank"><i class="fa fa-facebook" aria-hidden="true"></i> Share on Facebook</a>
    <a class="btn-social google-plus"  href="https://plus.google.com/share?url=/2017/03/xgboost-v1/" target="_blank"><i class="fa fa-google-plus" aria-hidden="true"></i> Share on Google+</a>
</div><!-- /.share-this -->

                    <div class="page-meta">
	<p>Updated <time datetime="2017-03-19T00:00:00Z" itemprop="datePublished">March 19, 2017</time></p>
</div><!-- /.page-meta -->
                </footer><!-- /.footer -->
                <!-- JiaThis Button BEGIN -->
<div class="jiathis_style_32x32">
    <a class="jiathis_button_weixin"></a>
    <a class="jiathis_button_tsina"></a>
    <a class="jiathis_button_tqq"></a>
    <a class="jiathis_button_qzone"></a>
    <a href="http://www.jiathis.com/share" class="jiathis jiathis_txt jtico jtico_jiathis" target="_blank"></a>
    <a class="jiathis_counter_style"></a>
</div>
<script type="text/javascript" src="http://v3.jiathis.com/code/jia.js" charset="utf-8"></script>
<!-- JiaThis Button END -->

                <aside>
                    <section class="comment">
<!-- baidu JIA -->
<div class="bdsharebuttonbox"><a href="#" class="bds_more" data-cmd="more"></a><a title="分享到QQ空间" href="#" class="bds_qzone" data-cmd="qzone"></a><a title="分享到新浪微博" href="#" class="bds_tsina" data-cmd="tsina"></a><a title="分享到腾讯微博" href="#" class="bds_tqq" data-cmd="tqq"></a><a title="分享到人人网" href="#" class="bds_renren" data-cmd="renren"></a><a title="分享到微信" href="#" class="bds_weixin" data-cmd="weixin"></a></div>
<script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"1","bdSize":"16"},"share":{}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];</script>
<!-- end of baidu JIA -->

<!-- 多说评论框 start -->
    <div class="ds-thread" data-thread-key="/2017/03/xgboost-v1/" data-title="XGBoost解读(1)--原理" data-url="/2017/03/xgboost-v1/"></div>
<!-- 多说评论框 end -->
<!-- 多说公共JS代码 start (一个网页只需插入一次) -->
<script type="text/javascript">
var duoshuoQuery = {short_name:"yxzf"};
    (function() {
        var ds = document.createElement('script');
        ds.type = 'text/javascript';ds.async = true;
        ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
        ds.charset = 'UTF-8';
        (document.getElementsByTagName('head')[0] 
         || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
    </script>
<!-- 多说公共JS代码 end -->


</section>

                </aside>
            </div><!-- /.content -->
        </div><!-- /.inner-wrap -->
        <div class="ads"><script src="//about.me/chengguang.shen"></script>
</div>
    </article><!-- ./wrap -->
</div><!-- /#main -->
<script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-57330114-1', 'auto');
      ga('send', 'pageview');
</script>

<script>
    var _hmt = _hmt || [];
    (function() {
           var hm = document.createElement("script");
             hm.src = "//hm.baidu.com/hm.js?138fcb8c5448d0003de5abef31cdd0b9";
               var s = document.getElementsByTagName("script")[0]; 
                 s.parentNode.insertBefore(hm, s);
                 })();
</script>


<script type="text/javascript"
     src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
     MathJax.Hub.Config({
       extensions: ["tex2jax.js","TeX/noErrors.js","TeX/AMSsymbols.js"],
       jax: ["input/TeX","output/HTML-CSS"],
       tex2jax: {
         inlineMath: [['$','$'],["\\(","\\)"]],
         displayMath: [['\\[','\\]'], ['$$','$$']],
         balanceBraces: true
       },
       TeX: {equationNumbers: {autoNumber: ["AMS"], useLabelIds: true}},
       "HTML-CSS": {availableFonts:["TeX"]},
       });
      MathJax.Hub.Queue(["Typeset",MathJax.Hub]);
</script>


      <footer role="contentinfo" id="site-footer">
	<nav role="navigation" class="menu bottom-menu">
		<ul class="menu-item">
		
      
        
      
			<li><a href="" ></a></li>
		
		</ul>
	</nav><!-- /.bottom-menu -->
	<p class="copyright">&#169; 2017 <a href="">YXZF's  Blog</a> powered by <a href="http://jekyllrb.com" rel="nofollow">Jekyll</a> + <a href="http://mmistakes.github.io/skinny-bones-jekyll/" rel="nofollow">Skinny Bones</a>.</p>
</footer>
    </div>

    <script src="/js/vendor/jquery-1.9.1.min.js"></script>
    <script src="/js/main.js"></script>
    
    

  </body>

</html>
